{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "prerequisite-pharmacology",
   "metadata": {},
   "source": [
    "# **Snakemake Demo**\n",
    "\n",
    "Author: Jimmy Liu\n",
    "\n",
    "Date: Feb 25th 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-stationery",
   "metadata": {},
   "source": [
    "## **Introduction**\n",
    "The purpose of this demo is to build upon the fundamentals of Snakemake and introduce additional concepts that may be useful for more advanced pipeline implementations. Thus far, we have seen how the snakemake workflow engine determines the flow of data from rule outputs and inputs, as well as the use of wildcards to circumvent the necessity of explicit file name definitions in every rule. Here, we will explore various methods to promote **flexability**, **generalization** and **modularization** of Snakemake pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjacent-budget",
   "metadata": {},
   "source": [
    "## **Getting Started**\n",
    "To install the necessary dependencies for this tutorial, first clone the GitHub repository followed by creating a new conda environment called `snakemake_demo2` by running the following commands in the terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abroad-times",
   "metadata": {},
   "outputs": [],
   "source": [
    "git clone https://github.com/jimmyliu1326/PHL_snakemake_demo.git\n",
    "cd PHL_snakemake_demo\n",
    "conda env create -f conda_env.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olive-presence",
   "metadata": {},
   "source": [
    "Next, activate the conda environment with the following command line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-brain",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda activate snakemake_demo2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chronic-ridge",
   "metadata": {},
   "source": [
    "### 1. Generalization and Pipeline Configurations\n",
    "* Data anlysis pipelines should be easily operable to readily process large-scale datasets\n",
    "* Common standardized methods to define and parse data inputs\n",
    "* Global config file to define pipeline parameters and variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-hawaiian",
   "metadata": {},
   "source": [
    "#### **Exercise 1.1**\n",
    "Using a global `config.yaml` file to define pipeline parameters and variables that are accessible to Snakefile by referencing the `config` object.\n",
    "\n",
    "Define `config.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-balloon",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples:\n",
    "    samples_1\n",
    "    samples_2\n",
    "amr_db: card"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-times",
   "metadata": {},
   "source": [
    "Define `Snakefile`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swedish-candy",
   "metadata": {},
   "outputs": [],
   "source": [
    "configfile: \"config.yaml\"\n",
    "\n",
    "rule all:\n",
    "    input:\n",
    "        expand(\"abricate/{samples}_abricate_res.tsv\", samples=config[\"samples\"])\n",
    "\n",
    "rule abricate:\n",
    "    input:\n",
    "        \"data/{samples}.fasta\"\n",
    "    output:\n",
    "        \"abricate/{samples}_abricate_res.tsv\"\n",
    "    params:\n",
    "        db=config[\"amr_db\"]\n",
    "    shell:\n",
    "        \"abricate --db {params.db} {input} > {output}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approximate-passage",
   "metadata": {},
   "source": [
    "#### **Exercise 1.2**\n",
    "Define pipeline configurations via command lines without the creation of `config.yaml`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-cleaners",
   "metadata": {},
   "source": [
    "Define `Snakefile`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approved-madonna",
   "metadata": {},
   "outputs": [],
   "source": [
    "rule all:\n",
    "    input:\n",
    "        expand(\"abricate/{samples}_abricate_res.tsv\", samples=config[\"samples\"])\n",
    "\n",
    "rule abricate:\n",
    "    input:\n",
    "        \"data/{samples}.fasta\"\n",
    "    output:\n",
    "        \"abricate/{samples}_abricate_res.tsv\"\n",
    "    params:\n",
    "        db=config[\"amr_db\"]\n",
    "    shell:\n",
    "        \"abricate --db {params.db} {input} > {output}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patent-vietnamese",
   "metadata": {},
   "source": [
    "Run the following command line that includes pipeline configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-romania",
   "metadata": {},
   "outputs": [],
   "source": [
    "snakemake --config samples=[sample_1,sample_2] amr_db=card --cores 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worthy-moscow",
   "metadata": {},
   "source": [
    "#### **Exercise 1.3**\n",
    "Instead of defining input data via `config.yaml`, an alternative method is to parse tabular input files, `samples.csv` and define the input dataset for analysis. A typical `samples.csv` has two columns: Sample Name and File Path; column headers are optional.\n",
    "\n",
    "Example `samples.csv`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-filter",
   "metadata": {},
   "source": [
    "sample_1,data/hello/sample_1.fasta\n",
    "sample_2,data/world/sample_2.fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distributed-sucking",
   "metadata": {},
   "source": [
    "Define `Snakefile`\n",
    "* Load the `samples.csv` as a pandas data frame\n",
    "* Rename the columns if the csv file is headerless\n",
    "* The list of sample names can be directly accessed from the pandas data frame\n",
    "* Use of a lambda function that takes the `wildcards` object as input and returns the `input file path` from the pandas data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-newcastle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "samples_file=config[\"samples\"]\n",
    "samples_tbl=pd.read_csv(samples_file, header=None)\n",
    "samples_tbl.columns=[\"Sample\", \"Path\"]\n",
    "samples_tbl=samples_tbl.set_index(\"Sample\", drop=False)\n",
    "\n",
    "rule all:\n",
    "    input:\n",
    "        expand(\"abricate/{samples}_abricate_res.tsv\", samples=samples_tbl.Sample)\n",
    "\n",
    "rule abricate:\n",
    "    input:\n",
    "        lambda wildcards: samples_tbl.Path[wildcards.samples]\n",
    "    output:\n",
    "        \"abricate/{samples}_abricate_res.tsv\"\n",
    "    params:\n",
    "        db=config[\"amr_db\"]\n",
    "    shell:\n",
    "        \"abricate --db {params.db} {input} > {output}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-lloyd",
   "metadata": {},
   "source": [
    "Execute the snakemake pipeline using the following command line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-diesel",
   "metadata": {},
   "outputs": [],
   "source": [
    "snakemake --config samples=samples.csv amr_db=card --cores 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dirty-closure",
   "metadata": {},
   "source": [
    "#### **Exercise 1.4**\n",
    "Instead of using lambda functions, an alternative method is to explicitly define a **helper function** that returns file path for a given sample in the pandas data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-mystery",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "samples_file=config[\"samples\"]\n",
    "samples_tbl=pd.read_csv(samples_file, header=None)\n",
    "samples_tbl.columns=[\"Sample\", \"Path\"]\n",
    "samples_tbl=samples_tbl.set_index(\"Sample\", drop=False)\n",
    "\n",
    "def abricate_input(wildcards):\n",
    "    sample=wildcards.samples\n",
    "    path=samples_tbl.Path[sample]\n",
    "    return path\n",
    "\n",
    "rule all:\n",
    "    input:\n",
    "        expand(\"abricate/{samples}_abricate_res.tsv\", samples=samples_tbl.Sample)\n",
    "\n",
    "rule abricate:\n",
    "    input:\n",
    "        abricate_input\n",
    "    output:\n",
    "        \"abricate/{samples}_abricate_res.tsv\"\n",
    "    params:\n",
    "        db=config[\"amr_db\"]\n",
    "    shell:\n",
    "        \"abricate --db {params.db} {input} > {output}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceramic-information",
   "metadata": {},
   "source": [
    "Execute the snakemake pipeline using the following command line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endangered-drilling",
   "metadata": {},
   "outputs": [],
   "source": [
    "snakemake --config samples=samples.csv amr_db=card --cores 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satellite-science",
   "metadata": {},
   "source": [
    "### 2. Flexibility and Conditionality\n",
    "* May not always want to execute every components in the pipeline every time\n",
    "* Empower end-users the flexibility to easily toggle pipeline flow and tailor the data analysis to their needs\n",
    "* Different input data types may require different branches of analysis and processing\n",
    "* Think of if..else statements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-wildlife",
   "metadata": {},
   "source": [
    "#### **Exercise 2.1**\n",
    "**Scenario:** We are interested in constructing Salmonella genome assemblies from raw Illumina paired-end reads and typically a QC step is ran to filter out low quality reads prior to assembly, but occasionally someone has already done the QC for us, so there isn't a need to re-run read QC again in our pipeline. Is it possible to make read QC an optional step in our pipeline?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-seventh",
   "metadata": {},
   "source": [
    "Define `snakefile`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overhead-parcel",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "samples_file=config[\"samples\"]\n",
    "samples_tbl=pd.read_csv(samples_file, header=None)\n",
    "samples_tbl.columns=[\"Sample\", \"R1\", \"R2\"]\n",
    "samples_tbl=samples_tbl.set_index(\"Sample\", drop=False)\n",
    "\n",
    "def spades_input_R1(wildcards):\n",
    "    sample=wildcards.samples\n",
    "    if config[\"QC\"] == 0:\n",
    "        return samples_tbl.R1[sample]\n",
    "    else:\n",
    "        return os.path.join(\"read_QC\", sample, sample+\"_R1_trimmed.fastq.gz\")\n",
    "\n",
    "def spades_input_R2(wildcards):\n",
    "    sample=wildcards.samples\n",
    "    if config[\"QC\"] == 0:\n",
    "        return samples_tbl.R2[sample]\n",
    "    else:\n",
    "        return os.path.join(\"read_QC\", sample, sample+\"_R2_trimmed.fastq.gz\")\n",
    "\n",
    "rule all:\n",
    "    input:\n",
    "        expand(\"spades/{samples}\", samples=samples_tbl.Sample)\n",
    "\n",
    "rule read_QC:\n",
    "    input:\n",
    "        R1=lambda wildcards: samples_tbl.R1[wildcards.samples],\n",
    "        R2=lambda wildcards: samples_tbl.R2[wildcards.samples]\n",
    "    output:\n",
    "        R1_trimmed=temp(\"read_QC/{samples}/{samples}_R1_trimmed.fastq.gz\"),\n",
    "        R2_trimmed=temp(\"read_QC/{samples}/{samples}_R2_trimmed.fastq.gz\"),\n",
    "        json=temp(\"read_QC/{samples}/fastp.json\"),\n",
    "        html=temp(\"read_QC/{samples}/fastp.html\")\n",
    "    threads: 4\n",
    "    shell:\n",
    "        \"\"\"\n",
    "        fastp -i {input.R1} -I {input.R2} -o {output.R1_trimmed} -O {output.R2_trimmed} \\\n",
    "            -w {threads} -j {output.json} -h {output.html}\n",
    "        \"\"\"\n",
    "\n",
    "rule spades:\n",
    "    input:\n",
    "        R1=spades_input_R1,\n",
    "        R2=spades_input_R2\n",
    "    output:\n",
    "        \"spades/{samples}\"\n",
    "    threads: 4\n",
    "    shell:\n",
    "        \"spades.py -1 {input.R1} -2 {input.R2} -o {output} -t {threads}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stunning-integration",
   "metadata": {},
   "source": [
    "Execute the following command line to **skip** the read QC step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "still-harassment",
   "metadata": {},
   "outputs": [],
   "source": [
    "snakemake --cores 4 --config samples=samples.csv QC=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-idaho",
   "metadata": {},
   "source": [
    "Execute the following command line to **run** the read QC step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-miami",
   "metadata": {},
   "outputs": [],
   "source": [
    "snakemake --cores 4 --config samples=samples.csv QC=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legal-brisbane",
   "metadata": {},
   "source": [
    "#### **Exercise 2.2**\n",
    "**Scenario:** What if we have numerous samples of which only **some** raw reads (fastq) files require read QC. One simple solution is to create two different `samples.csv` files and use the snakemake pipeline written in `Exercise 2.1` using different parameters `QC=0/1` to analyze the samples separately. However, is it possible to implement conditional execution of certain components of the pipeline depending on the input type such that all samples can be processed in a single snakemake run?\n",
    "\n",
    "Define `snakefile`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiac-blast",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "samples_file=config[\"samples\"]\n",
    "samples_tbl=pd.read_csv(samples_file, header=None)\n",
    "samples_tbl.columns=[\"Sample\", \"R1\", \"R2\", \"Trim\"]\n",
    "samples_tbl=samples_tbl.set_index(\"Sample\", drop=False)\n",
    "\n",
    "def spades_input_R1(wildcards):\n",
    "    sample=wildcards.samples\n",
    "    if samples_tbl.Trim[sample] == \"notrim\":\n",
    "        return samples_tbl.R1[sample]\n",
    "    else:\n",
    "        return os.path.join(\"read_QC\", sample, sample+\"_R1_trimmed.fastq.gz\")\n",
    "\n",
    "def spades_input_R2(wildcards):\n",
    "    sample=wildcards.samples\n",
    "    if samples_tbl.Trim[sample] == \"notrim\":\n",
    "        return samples_tbl.R2[sample]\n",
    "    else:\n",
    "        return os.path.join(\"read_QC\", sample, sample+\"_R2_trimmed.fastq.gz\")\n",
    "\n",
    "rule all:\n",
    "    input:\n",
    "        expand(\"spades/{samples}\", samples=samples_tbl.Sample)\n",
    "\n",
    "rule read_QC:\n",
    "    input:\n",
    "        R1=lambda wildcards: samples_tbl.R1[wildcards.samples],\n",
    "        R2=lambda wildcards: samples_tbl.R2[wildcards.samples]\n",
    "    output:\n",
    "        R1_trimmed=temp(\"read_QC/{samples}/{samples}_R1_trimmed.fastq.gz\"),\n",
    "        R2_trimmed=temp(\"read_QC/{samples}/{samples}_R2_trimmed.fastq.gz\"),\n",
    "        json=temp(\"read_QC/{samples}/fastp.json\"),\n",
    "        html=temp(\"read_QC/{samples}/fastp.html\")\n",
    "    threads: 4\n",
    "    shell:\n",
    "        \"\"\"\n",
    "        fastp -i {input.R1} -I {input.R2} -o {output.R1_trimmed} -O {output.R2_trimmed} \\\n",
    "            -w {threads} -j {output.json} -h {output.html}\n",
    "        \"\"\"\n",
    "\n",
    "rule spades:\n",
    "    input:\n",
    "        R1=spades_input_R1,\n",
    "        R2=spades_input_R2\n",
    "    output:\n",
    "        \"spades/{samples}\"\n",
    "    threads: 4\n",
    "    shell:\n",
    "        \"spades.py -1 {input.R1} -2 {input.R2} -o {output} -t {threads}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-contractor",
   "metadata": {},
   "source": [
    "Execute the following command line to **run** the Snakemake pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knowing-dakota",
   "metadata": {},
   "outputs": [],
   "source": [
    "snakemake --cores 4 --config samples=samples.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serial-program",
   "metadata": {},
   "source": [
    "### 3. Modularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integral-chapter",
   "metadata": {},
   "source": [
    "Here we will explore the ability of Snakemake to integrate rules from multiple independent files (.smk) that can be reused for different pipeline implementations.\n",
    "\n",
    "**Advantages of Modularization:**\n",
    "* Improves readability of pipeline code\n",
    "* Common analysis methods can be written as independent modules to facilitate easy integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varied-gender",
   "metadata": {},
   "source": [
    "#### **Exercise 3.1**\n",
    "We will use the `Snakefile` in `Ex2.1` as an example and attempt to decouple the rules into independent modules. The original `Snakefile` can be reduced down to simply contain the default target rule that specifies the final output of interest in our pipeline which is genome assemblies. All the other rules such as readQC and Spades assembly can be written as separate .smk files representing independent modules that can be readily loaded by other Snakefiles using `include`. The purpose of `common.smk` is to maintain a list of useful helper functions and parse data inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-canberra",
   "metadata": {},
   "source": [
    "Define `Snakefile`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "german-deviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load modules\n",
    "include: \"rules/common.smk\"\n",
    "include: \"rules/readQC.smk\"\n",
    "include: \"rules/assembly.smk\"\n",
    "\n",
    "rule all:\n",
    "    input:\n",
    "        expand(\"spades/{samples}\", samples=samples_tbl.Sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
